// <copyright file="DefaultApiController.cs" company="DECTech.Tokyo">
// Copyright (c) DECTech.Tokyo. All rights reserved.
// </copyright>

/*
 * Crawler API
 *
 * API for crawling web pages and searching in crowled result
 *
 * OpenAPI spec version: 1.0.0
 * Contact: zakhar_amirov@dectech.tokyo
 * Generated by: https://github.com/swagger-api/swagger-codegen.git
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

namespace CrawlerApi.Controllers
{
    using System;
    using System.Collections.Async;
    using System.Collections.Generic;
    using System.ComponentModel.DataAnnotations;
    using System.IO;
    using System.Linq;
    using System.Net.Mime;
    using System.Threading.Tasks;
    using Azure.Storage;
    using CrawlerLib;
    using CrawlerLib.Data;
    using CrawlerLib.Grabbers;
    using CrawlerLib.Metadata;
    using Microsoft.AspNetCore.Mvc;
    using Models;
    using Swashbuckle.SwaggerGen.Annotations;
    using SessionState = Models.SessionState;

    /// <summary>
    /// Main API controller
    /// </summary>
    public sealed class DefaultApiController : Controller
    {
        private readonly ICrawlerStorage crawlerStorage;
        private readonly IDataStorage storage;

        /// <summary>
        /// Initializes a new instance of the <see cref="DefaultApiController" /> class.
        /// </summary>
        /// <param name="storage">Azure storage helper class.</param>
        /// <param name="crawlerStorage">Crawler storage.</param>
        public DefaultApiController(IDataStorage storage, ICrawlerStorage crawlerStorage)
        {
            this.storage = storage;
            this.crawlerStorage = crawlerStorage;
        }

        /// <summary>adds or replaces metadata parsing parameters</summary>
        /// <param name="parserParameters">Parameters.</param>
        /// <response code="200">parser added</response>
        /// <response code="400">invalid parameters</response>
        /// <returns>A <see cref="Task" /> representing the asynchronous operation.</returns>
        [HttpPost]
        [Route("/CrawlerApi/1.0.0/parser")]
        [SwaggerOperation("AddParser")]
        [SwaggerResponse(200)]
        public async Task AddParser([FromBody] ParserParameters parserParameters)
        {
            if (parserParameters == null)
            {
                throw new ArgumentNullException(nameof(parserParameters));
            }

            if (parserParameters.OwnerId == null)
            {
                throw new ArgumentNullException(nameof(parserParameters.OwnerId));
            }

            await storage.InsertOrReplaceAsync(parserParameters);
        }

        /// <summary>get list of crawling sessions information by list of ids</summary>
        /// <param name="ownerId">Session owner id.</param>
        /// <param name="sessionIds">Collection of session ids.</param>
        /// <param name="pagesize">Number of items in a page. 10 by default.</param>
        /// <param name="requestId">Paginated request id for continuation.</param>
        /// <response code="200">list of selected sessions information</response>
        /// <response code="400">wrong parameters</response>
        /// <returns>Pagination with collection of sessions info.</returns>
        [HttpGet]
        [Route("/CrawlerApi/1.0.0/incite")]
        [SwaggerOperation("GetIncites")]
        [SwaggerResponse(200, type: typeof(Paged<Session>))]
        public async Task<IActionResult> GetIncites(
            [FromQuery] [Required] string ownerId = null,
            [FromQuery] List<string> sessionIds = null,
            [FromQuery] int? pagesize = null,
            [FromQuery] string requestId = null)
        {
            if (ownerId == null)
            {
                throw new ArgumentNullException(nameof(ownerId));
            }

            var page = await crawlerStorage.GetSessions(
                           ownerId,
                           sessionIds,
                           pagesize ?? 10,
                           requestId);
            var sessionsTasks = page.Items.Select(async sess =>
            {
                var urls = await crawlerStorage.GetSessionUris(sess.Id).ToListAsync();
                var sessionUris = urls.Select(u => new SessionUri(u.Uri, HttpStateToSessionState(u.State)));
                return new Session(sess.Id, sessionUris.ToList(), ToSessionState(sess.State));
            });

            var sessions = await Task.WhenAll(sessionsTasks);

            var pagination = new Paged<Session>(sessions, page.RequestId);

            return new ObjectResult(pagination);
        }

        /// <summary>download crawled page</summary>
        /// <param name="ownerId">Blob onwer id.</param>
        /// <param name="uri">Page URI.</param>
        /// <response code="200">responce with page content</response>
        /// <response code="400">invalid input</response>
        /// <returns>Page content.</returns>
        [HttpGet]
        [Route("/CrawlerApi/1.0.0/page")]
        [SwaggerOperation("GetPage")]
        [SwaggerResponse(200, type: typeof(byte[]))]
        public async Task<IActionResult> GetPage([FromQuery] string ownerId = null, [FromQuery] string uri = null)
        {
            if (ownerId == null)
            {
                throw new ArgumentNullException(nameof(ownerId));
            }

            if (uri == null)
            {
                throw new ArgumentNullException(nameof(uri));
            }

            var stream = new MemoryStream(); // TODO remove it rewriting to Response.
            await crawlerStorage.GetUriContet(ownerId, uri, stream);
            stream.Position = 0;
            return new FileStreamResult(stream, MediaTypeNames.Application.Octet);
        }

        /// <summary>get metadata parsing parameters set</summary>
        /// <param name="ownerId">Parsers owner Id.</param>
        /// <param name="parserIds">Custom parsers Ids.</param>
        /// <response code="200">parsers collection</response>
        /// <response code="400">invalid parameters</response>
        /// <returns>Collection of parsers settings.</returns>
        [HttpGet]
        [Route("/CrawlerApi/1.0.0/parser")]
        [SwaggerOperation("GetParsers")]
        [SwaggerResponse(200, type: typeof(IList<ParserParameters>))]
        public async Task<IActionResult> GetParsers(
            [FromQuery] [Required] string ownerId = null,
            [FromQuery] List<string> parserIds = null)
        {
            if (ownerId == null)
            {
                throw new ArgumentNullException(nameof(ownerId));
            }

            var result = new List<ParserParameters>();

            foreach (var id in parserIds)
            {
                var pp = await storage.RetreiveAsync(new ParserParameters(ownerId, id));
                result.Add(pp);
            }

            return new ObjectResult(result);
        }

        /// <summary>starts crawling</summary>
        /// <param name="configuration">crawler configuration</param>
        /// <response code="200">crawling started</response>
        /// <response code="400">invalid input</response>
        /// <returns>Session id.</returns>
        [HttpPost]
        [Route("/CrawlerApi/1.0.0/incite")]
        [SwaggerOperation("Incite")]
        [SwaggerResponse(200, type: typeof(string))]
        public async Task<IActionResult> Incite([FromBody] CrawlerConfiguration configuration)
        {
            var config = new Configuration
                         {
                             Storage = crawlerStorage,
                             Depth = 0,
                             HostDepth = 0,
                             MetadataExtractors = new IMetadataExtractor[]
                                                  {
                                                      new RdfaMetadataExtractor(),
                                                      new MicrodataMetadataExtractor()
                                                  }
                         };

            config.HttpGrabber = new WebDriverHttpGrabber(config);

            var crawler = new Crawler(config);

            var session = await crawler.InciteStart(
                              configuration.OwnerId,
                              configuration.Uris.Select(u => new Uri(u)));

            return new ObjectResult(session.SessionId);
        }

        private static SessionState HttpStateToSessionState(int state)
        {
            switch (state)
            {
                case 0:
                    return SessionState.NotStarted;
                case 200:
                    return SessionState.Done;
                default:
                    return SessionState.Error;
            }
        }

        private SessionState ToSessionState(CrawlerLib.Data.SessionState sessState)
        {
            switch (sessState)
            {
                case CrawlerLib.Data.SessionState.NotStarted:
                    return SessionState.NotStarted;
                case CrawlerLib.Data.SessionState.InProcess:
                    return SessionState.InProcess;
                case CrawlerLib.Data.SessionState.Done:
                    return SessionState.Done;
                case CrawlerLib.Data.SessionState.Error:
                    return SessionState.Error;
                default:
                    throw new ArgumentOutOfRangeException(nameof(sessState), sessState, null);
            }
        }
    }
}